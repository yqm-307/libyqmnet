# 打卡记录

## 前言：
        大三菜鸡，正在实习，趁空档学习一下网络编程。刚入门就看网上推荐看陈硕大佬的 muduo 网络库，
    所以就想着看一遍，并想办法优化或者复刻。在这里记录自己每天的感想和进度。

## 2月4日
        开始动笔写这个的东西的时候，muduo已经开始看，并且写了一些组件了。现在就是已经完成了定时器、
    日志。因为是自实现的定时器，而不是 timefd 那个linux 系统库的定时器。
    
#### 定时器
        timefd的话，好像是一个内存中的文件，这个文件也会可读，触发可读事件就是超时了。所以很适合在
    select、poll、epoll中使用。而且muduo的Epoller刚好将所有事件放在一起管理，这样所有事件（包括各种回调和定时器事件）都会在EventLoop中处理了。

        自己实现的定时器就很简单，依靠的是c++11 的std::thread 中的 sleep_for 函数，将线程休眠，苏醒后就触发回调。

#### 定时器队列
        其实我更愿意称为定时事件队列，因为看了muduo里面的定时器后发现，其实整个队列里面最重要的结
    构就是一个有序队列（平衡二叉树），里面存的键值对就是TimerOutCallBack和timestamp。按照
    timestamp排序，也就是按照时间排序。但是内部只有一个定时器在工作，这是个什么原理呢？

        就是有定时事件的时候就放进队列排队，然后使用定时器，定时器到时就会抛给EventLoop中进行处理。接着，定时器开始做队列中的下一个任务。


#### 日志
        这个没什么好说的，就是简单的做了一个队列化。提供一个异步接口，写日志不会阻塞在write操作上
    ，因为开了个磁盘IO线程不断写。


## 2月8日
        buffer的设计遵循了陈硕大佬的设计风格。缓冲区内有一个动态数组 vector<char> 作
    为byte流，另外两个readindex和writeindex，代表当前读和当前写的字节数。
    
    这个buffer在写的时候，是这样的
    初始状态：
    [------------------]
    ^
    read
    write
    
    写入字节：
    [------------------]
    ^       ^
    read    write

    读取字节：
    [------------------]
            ^    ^
            read write

    向前移动：
    [------------------]
    ^    ^
    read write


## 2月11日
        今天看我自己写的定时器队列，其实和陈硕大佬写的不一样。muduo原本的定时器使用的是timerfd，可以配
    合epoll使用，根据事件触发，不需要轮询等待下一个定时器事件。
        但是我自己写的就比较笨了，我是自己尝试实现的timer，采用的是thread::sleep_for()。而且今天发现
    我的线程还不可以复用，就是每有一个事件发生就会开一个线程，用完销毁，所以销毁和创建线程可以通过轮询替
    代。但是效率是否会提高还需要等待测试
        muduo对于周期性任务，执行完毕后会插入金timerqueue，确实牛皮，我想了半天都没想到怎么解决一个定时
    事件的问题。

## 2月14日
        看了EventLoop和Channel，梳理好了他们之间的关系。Channel有点不太懂，大致是对一个事件及其相关信
    息的封装。
        EventLoop就是主要的事件循环。但是不是这个loop不是一直出于工作状态，如果闲置的话会sleep，等待任务
    使用了eventfd来做通信，外部可以wakeup和sleep这个循环。
        同时修复了timerqueue测试案例出现bug的问题。

## 2月15日
        决定修改timerqueue，将它和eventloop分离。效率上来说不会太差，但是逻辑上可能更复杂。


## 2月16日
        给logger出队操作加锁，之前以为不加锁没影响，但是肯定还是要加锁的，之前测试开100个线程没问题，现
    在重新测试开1000个线程竞争就激烈了
        今天在研究cmake，遇到问题：没法编译通过程序

## 2月20日
        线程池大致完成，开启固定数量线程，线程根据任务队列中数据是否空阻塞，如果有新的task加入，就唤醒一个
    线程。但是遇到了奇怪的bug，暂时定位在Thread中的IsBlock和Block函数。


## 2月21日
        线程池bug已经解决，问题是因为有新任务到达时唤醒thread的时候调用isBlock出现问题，我用gdb单步调试，
    发现是空指针问题。因为我使用vector保存Thread* ，但是初始化的时候使用的是for_each 遍历vector。但是忘
    记给vector赋初始值了，后来改成push_back就没问题了。
        EventLoop大致没有了问题，现在需要开始封装Acceptor和TcpConnection，Acceptor负责监听和新连接到达
    时的回调，TcpConnection负责数据收发和回调。
        当前线程挂起和唤醒，都是立即的，如果改成有个缓冲时间可能会好很多。这样就不会频繁的调用notify和wait
        但是如果task入队快于线程池处理，就不会频繁的阻塞。所以还是不改了

## 2月26日
        我是傻逼，有个if后面加了引号坑了我一天，我超

## 2月28日
        epoll出现了问题，可能是sockfd没有设置复用，回学校再研究一下。今天调试一下看看怎么回事，初步定位一下
    回学校再详细看。准备开润。


## 3月5日
        编写了一个echo例程，网络库基础功能测试结束。重写cmake，将网络库编译成静态库，并编写example的cmake-
    list
        看了看shell脚本的语法，有点怪，写起来细节非常多
        昨天看了一天MySQL，速成一下，几天就可以了

## 3月10日
        看完了mysql，网络库还是有一些bug，有时间定位一下问题。然后看一下redis，这个网络库也只是练手用的，准
    备学asio了，毕竟要学会使用一个成熟的网络库。

## 3月17日
        log部分有些问题，程序崩溃原因是不同进程尝试打开一个文件，再尝试对一个文件写的时候就核心转储了。于是重
    写了log的构造，getinstance()创建单例的时候指定文件名
        
        log解决了之后，TcpClient部分也出现了问题，connect失败，有时间需要解决一下。
        
        开始编写单元测试程序，今天写了thread和threadpool

## 3月23日
        最近在看boost、asio和libco，稍微了解了一下基础用法，等后续有时间看看源码。
        
        今天发现了buffer有bug，要先做好单元测试，确保每个组件都没有大问题。

        网络库暂时放一放，先润去写语音服务器了

## 3月31日
        咕咕~

## 5月7日
        debug 修改buffer字节位移操作问题
        todo  buffer部分单元测试
        修改部分变量名称

        todo 模拟客户端程序异常崩溃需要debug

## 5月8日
        修复TcpClient崩溃问题。echo测试无问题。
        修改TcpConnection send逻辑错误
        修改EventLoop注释
        修改TimeTask is_cannecl 为atomic_bool
        修改EchoClient例程

## 5月10日
        修改 EventLoop 的 runEveny系列函数，原本返回TimeTask* ，但是考虑到 timetask(定时任务) 生命期由timequeue管理，所以不应该保露 timetask 。就修改为返回句柄
        同步修改TimeTask
        同步修改TimerQueue
        同步修改 EventLoop 接口
        添加 CallBack 中关于 timetask 句柄 -- timetask_t 的typedef

        添加文件 timerqueue 测试例程

## 5月13日
        修复TcpConnection send 中 strlen计算错误问题
        todo 定时器队列优化
            +1 当前方案：轮询 ，无间歇轮询是否有定时事件触发。
            
            +2 方案一：时间轮，尚不了解
            +3 方案二：使用timefd 这样还可以用 libco 的co_eventloop，或者直接在eventloop中管理。但是引入协程会hook系统函数。
        
        研究协程发现，libco是不需要调度器的，在单线程内写同步逻辑就可以，每个线程都有一个epoll，这样TcpConnection就可以独立在这些协程中管理。
        后续的话写几个demo试验一下。

## 5月14日
        增加DeCode、EnCode处理。默认 send、recv 为原始数据，需要注册回调函数。
        todo 添加协程在 Eventloop 触发 channel 的 doTask 时 ，创建协程read和write。这样就可以在当前线程挂起io协程。
            但是可能会影响EventLoop 的循环？
            同时 libco 是非对称协程，还需要 resume 和 yield，尚未想好逻辑。

## 5月15日
        发现定时器bug，顺便启动备用计划，筹备时间轮定时器。
            当前定时器实现bug在于sleep时，有新的定时任务到达，无法计时唤醒并更新sleep中的定时器事件。或许可以通过cond实现，但是选择使用新的设计
        --时间轮定时器队列。
        添加install.sh 一键构建libyqmnet


## 5月18日
        现在定时器更改为基于升序链表的定时器队列。
        todo Config 配置文件

## 5月25日
        日志修改为，双缓冲日志。char数组加循环链表（vector映射的伪链表）
        测试结果：百万条日志写入时间：原本平均在3000ms左右。双缓冲日志，缩短到2000ms。优化30%以上

## 5月27日
        debug：timerqueue中初始化timer自动启动timer循环。修改为初始化之后，在传给timertimequeue指针。否则可能出现，timequeue尚未构建完成就被timer 调用到tickcallback
        修改build.sh

## 5月28日
        封装pthread线程同步原语。
        替换项目里的线程同步原语。
        todo：将EventLoop封装到threadpool 中，可以无脑初始化，提供一个对外接口 addtask 向线程池中任务队列 加入任务，由任务调度器分配任务。
        todo：封装万threadpool之后，编写多线程服务器的测试例程。
